import numpy as np
lst1=[3.14,2.17,0,1,2]
nd1=np.array(lst1)
print(nd1)
#嵌套列表转换为多维数组
lst2=[[3.14,2.17,0,1,2],[1,2,3,4,5]]
nd2=np.array(lst2)
print(nd2)
print('生成形状（4，4），值在0-1之间的随机数：')
print(np.random.random((4,4)),end='\n\n')
#产生一个一个取值范围在【1，50）之间数组。形状(3,3)
print(np.random.randint(low=1,high=50,size=(3,3)),end='\n\n')
#产生数组元素是均匀分布的随机数
print(np.random.uniform(low=1,high=3,size=(3,3)),end='\n\n')
#生成满足正态分布的形状为（3，3）矩阵
print(np.random.randn(3,3))
np.random.seed(10)
#按指定随机种子第一次生成随机数
print(np.random.randint(1,5,(2,2)))
np.random.seed(10)
print(np.random.randint(1,5,(2,2)))
#全0的3*3矩阵
nd5=np.zeros([3,3])
#与nd5形状一样
np.zeros_like(nd5)
#全是1的3*3矩阵
nd6=np.ones([3,3])
#3阶单位矩阵
nd7=np.eye(3)
#3阶对角矩阵
nd8=np.diag([1,2,3])
#数据暂时保存
nd9=np.random.random([5,5])
np.savetxt(X=nd9,fname='./text1.txt')
nd10=np.loadtxt('./text1.txt')
np.random.seed(2019)
nd11=np.random.random([10])
#获取第四个数据
nd11[3]
#截取一段数据
nd11[3:6]
#截取固定间隔数据
nd11[1:6:2]
#倒序取数
nd11[::-2]
#截取多维数组某个区域内容
nd12=np.arange(25).reshape([5,5])
nd12[1:3,1:3]
#截取某个值域之内的数据
nd12[(nd12>3)&(nd12<10)]
#读取2，3行
nd12[0:3, :]
#读取指定列
nd12[:,1:3]
from numpy import random as nr
#size指定输出数组形状
a=np.arange(1,25,dtype=float)
c1=nr.choice(a,size=(3,4))#随机可重复抽取
c2=nr.choice(a,size=(3,4),replace=False)#默认True可重复抽取，随机不重复抽取
c3=nr.choice(a,size=(3,4),p=a / np.sum(a))#随机但按制度概率抽取
arr=np.arange(10)
print(arr)
#变为2行5列
print(arr.reshape(2,5))
#指定行或列，其他用-1代替
print(arr.reshape(5,-1))
arr=np.arange(10)
#变为2行5列
arr.resize(2,5)
arr=np.arange(6).reshape(2,-1)
#按照列优先展平
arr.ravel('F')
#按照行优先展平
arr.ravel()
#矩阵转换为向量，默认行优先（order=c）
a=np.floor(10*np.random.random((3,4)))
a.flatten(order='C')
#squeeze主要用于降维把矩阵含1的维度去掉
arr=np.arange(3).reshape(3,1)
arr.squeeze().shape#(3,)
arr1=np.arange(6).reshape(3,1,2,1)
arr1.squeeze().shape
#transpose用于高维矩阵轴对换
arr2=np.arange(24).reshape(2,3,4)
arr2.transpose(1,2,0).shape#(3, 4, 2)
#合并数组
a=np.array([1,2,3])
b=np.array([4,5,6])
c=np.append(a,b)
#合并多维数组
a=np.arange(4).reshape(2,2)
b=np.arange(4).reshape(2,2)
c=np.append(a,b,axis=0)#按行合并
c.shape#(4, 2)
d=np.append(a,b,axis=1)#按列合并
d.shape#(2, 4)
#沿指定轴连接数组或矩阵
a=np.arange(4).reshape(2,2)
b=np.arange(4).reshape(2,2)
c=np.concatenate((a,b),axis=0)
d=np.concatenate((a,b),axis=1)
#沿指定轴堆叠数组或矩阵
a=np.array([[1,2],[3,4]])
b=np.array([[5,6],[7,8]])
np.stack((a,b),axis=0)
#zip用于张量计算
a1=[1,2,3]
b1=[4,5,6]
c1=zip(a1,b1)
for i,j in c1:
    print(i,end=",")
    print(j)
data_train=np.random.randn(10000,2,3)
np.random.shuffle(data_train)#打乱这10000数据
batch_size=100#定义批量大小
#批处理
for i in range(0,len(data_train),batch_size):
    x_batch_sum=np.sum(data_train[i:i+batch_size])
    print("第{}批次，该批次数据之和：".format(i,x_batch_sum))
import torch
import numpy as np
x=torch.tensor([1,2])
y=torch.tensor([3,4])
z=x.add(y)
z
x
x.add_(y)
x
torch.Tensor([1,2,3,4,5,6,7,8])
#指定形状
torch.Tensor(2,3)
t=torch.Tensor([[1,2,3],[4,5,6]])
t.size()
torch.Tensor(t.size())
#单位矩阵
torch.eye(2,2)
#根据规则生成数据
torch.linspace(1,10,4)
#满足均匀分布随机数
torch.rand(2,3)
#标准分布
torch.randn(2,3)
#数据相同值全为0
torch.zeros_like(torch.rand(2,3))
x=torch.randn(2,3)
x.size()
x.dim()#2
x.view(3,2)
y=x.view(-1)#x展平为1维向量
y.shape
#添加一个维度
z=torch.unsqueeze(y,0)
z.size()
z.numel()
torch.manual_seed(100)
x=torch.randn(2,3)
x[0,:]#第一行所有数据
x[:,-1]#最后一列数据
mask=x>0
torch.masked_select(x,mask)#获取>0值
torch.nonzero(mask)#获取非0下标。即行列索引
#获取指定索引对应的值，输出一下规则得到
#out[i][j]=input[index[i][j][j]]# if dim==0
#out[i][j]=input[i][index[i][j]]#if dim==1
index=torch.LongTensor([[0,1,1]])
torch.gather(x,0,index)
index=torch.LongTensor([[0,1,1]])
a=torch.gather(x,1,index)
z=torch.zeros(2,3)
z.scatter_(1,index,a)
A=np.arange(0,40,10).reshape(4,1)
B=np.arange(0,3)
A1=torch.from_numpy(A)
B1=torch.from_numpy(B)
c=A1+B1
t=torch.randn(1,3)
t1=torch.randn(3,1)
t2=torch.randn(1,3)
torch.addcdiv(t,t1,t2,value=0.1)#t+0.1*(t1/t2)
torch.sigmoid(t)
torch.clamp(t,0,1)#t限制在0-1之间
t.add_(2)
a=torch.linspace(0,10,6)
a=a.view((2,3))
#沿y轴方向累加
b=a.sum(dim=0)
#沿y轴方向累加 并保留含1的维度
b=a.sum(dim=0,keepdim=True)
x=torch.randint(10,(2,3))
y=torch.randint(6,(3,4))
torch.mm(x,y)#二维点积
x=torch.randint(10,(2,2,3))
y=torch.randint(6,(2,3,4))
torch.bmm(x,y)#三维点积
x=torch.Tensor([2])
w=torch.randn(1,requires_grad=True)
b=torch.randn(1,requires_grad=True)
#正向传播
y=torch.mul(w,x)#w*x
z=torch.add(y,b)
x=torch.tensor([[2,3]],dtype=torch.float,requires_grad=True)
#初始化雅可比
J=torch.zeros(2,2)
#初始化目标张量
y=torch.zeros(1,2)
#定义y与x之间映射关系
#y1=x1**2+3*x2,y2=x2**2+2*x1
y[0,0]=x[0,0]**2+3**x[0,1]
y[0,1]=x[0,1]**2+2*x[0,0]
#y1对x梯度
y.backward(torch.Tensor([[1,0]]),retain_graph=True)
J[0]=x.grad
#x梯度清零
x.grad=torch.zeros_like(x.grad)
#y2对x梯度
y.backward(torch.Tensor([[0,1]]))
J[1]=x.grad
J
#detach切断一些分支反向传播
x=torch.ones(2,requires_grad=True)
y=x**2+3
#分离变量y生成新变量c
c=y.detach()
z=c*x
z.sum().backward()
x.grad==c
x.grad
c.grad_fn==None
c.requires_grad
x.grad.zero_()
y.sum().backward()
x.grad==2*x     
%matplotlib inline
from matplotlib import pyplot as plt
np.random.seed(100)
x=np.linspace(-1,1,100).reshape(100,1)
y=3*np.power(x,2)+2+0.2*np.random.rand(x.size).reshape(100,1)
plt.scatter(x,y)
plt.show()
#随机初始化参数
w1=np.random.rand(1,1)
b1=np.random.rand(1,1)
lr=0.001
for i in range(800):
    #正向
    y_pred=np.power(x,2)*w1+b1
    loss=0.5*(y_pred-y)**2
    loss=loss.sum()
    grad_w=np.sum((y_pred-y)*np.power(x,2))
    grad_b=np.sum((y_pred-y))
    w1-=lr*grad_w
    b1-=lr*grad_b
plt.plot(x,y_pred,'r-',label='predict',linewidth=4)
plt.scatter(x,y,color='blue',marker='o',label='true')
plt.xlim(-1,1)
plt.ylim(2,6)
plt.legend()
plt.show()
print(w1,b1)
torch.manual_seed(100)
dtype=torch.float
x=torch.unsqueeze(torch.linspace(-1,1,100),dim=1)#形状100*1
y=3*x.pow(2)+2+0.2*torch.rand(x.size())
#把tensor转换为numnpy
plt.scatter(x.numpy(),y.numpy())
plt.show()
w=torch.randn(1,1,dtype=dtype,requires_grad=True)
b=torch.zeros(1,1,dtype=dtype,requires_grad=True)
lr=0.001
for i in range(800):
    y_pred=x.pow(2).mm(w)+b
    loss=0.5*(y_pred-y)**2
    loss=loss.sum()
    loss.backward()
    #手动更新参数，用torch.no_grad()
    with torch.no_grad():
        w-=lr*w.grad
        b-=lr*b.grad
    #autograd计算梯度会自动累加到grad中，每次循环清零
    w.grad.zero_()
    b.grad.zero_()
plt.plot(x.numpy(),y_pred.detach().numpy(),'r-',label='predict',linewidth=4)
plt.scatter(x.numpy(),y.numpy(),color='blue',marker='o',label='true')
plt.xlim(-1,1)
plt.ylim(2,6)
plt.legend()
plt.show()
print(w,b)
import torch.nn as nn
#损失函数以及优化器
loss_func=nn.MSELoss()
optimizer=torch.optim.SGD([w,b],lr=0.001)
#训练模型
for i in range(10000):
    y_pred=x.pow(2).mm(w)+b
    loss=loss_func(y_pred,y)
    loss.backward()#自动计算梯度
    optimizer.step()
    optimizer.zero_grad()
plt.plot(x.numpy(),
y_pred.detach().numpy(),'r-',label='predict',linewidth=4)
plt.scatter(x.numpy(),y.numpy(),color='blue',marker='o',label='true')
plt.xlim(-1,1)
plt.ylim(2,6)
plt.legend()
plt.show()
print(w,b)
'''y_pred 是模型的输出，通常是通过一系列计算操作（如卷积、全连接层、激活函数等）从输入 x 得到的。这些操作会被 PyTorch 的自动微分机制记录下来，形成一个计算图。y_pred 作为计算图的一部分，它与输入张量、模型参数以及整个计算过程是关联的。
如果你直接将 y_pred 转换为 NumPy 数组（即 y_pred.numpy()），而没有调用 detach()，那么：
计算图会被修改：PyTorch 会尝试将 NumPy 数组的操作也加入到计算图中，这可能会导致计算图变得复杂，甚至引发错误。
内存占用问题：y_pred 与计算图关联时，PyTorch 会保留其梯度信息，这会占用额外的内存。如果只是用于绘图，这些梯度信息是不必要的。
x 并不参与模型的梯度计算，因为它是一个独立的输入，而不是通过计算图生成的。
'''
#构建数据迭代器
def data_iter(features,labels,batch_size=4):
    num_examples=len(features)
    indices=list(range(num_examples))#数据集中样本索引
    np.random.shuffle(indices)#样本读取顺序随机
    for i in range (0,num_examples,batch_size):
        indexs=torch.LongTensor(indices[i:min(i+batch_size,num_examples)])
        yield features.index_select(0,indexs),labels.index_select(0,indexs)
#训练模型
for i in range(1000):
    for features,labels in data_iter(x,y,10):
        y_pred=features.pow(2).mm(w)+b
        loss=loss_func(y_pred,labels)
        loss.backward()
        optimizer.step()#更新参数
        optimizer.zero_grad()
y_p=x.pow(2).mm(w).detach().numpy()+b.detach().numpy()
plt.plot(x.numpy(),y_p,'r-',label='predict',linewidth=4)
plt.scatter(x.numpy(),y.numpy(),color='blue',marker='o',label='true')
plt.xlim(-1,1)
plt.ylim(2,6)
plt.legend()
plt.show()
print(w,b)   
