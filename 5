#均方误差损失
import torch
import torch.nn as nn
import torch.nn.functional as F
torch.manual_seed(10)
loss=nn.MSELoss(reduction='mean')
input=torch.randn(1,2,requires_grad=True)
print(input)
target=torch.randn(1,2,requires_grad=True)
print(target)
output=loss(input,target)
print(output)
output.backward()
torch.manual_seed(10)
loss=nn.CrossEntropyLoss()
#假设类别数5
input=torch.randn(3,5,requires_grad=True)
#每个样本对应的类别索引，其值范围为[0,4]
target = torch.empty(3, dtype=torch.long).random_(5)
output=loss(input,target)
output.backward()
#动量算法代码
def sgd_momentum(parameters,vs,lr,gamma):
    for param,v in zip(parameters,vs):
        v[:]=gamma*v+lr*param.grad#更新速度变量
        param.data-=v#更新参数
        param.grad.data.zero_()
#NAG算法
def sgd_nag(parameters,vs,lr,gamma):
    for param,v in zip(parameters,vs):
        v[:]*=gamma
        v[:]-=lr*param.grad
        param.data+=gamma*gamma*v
        param.data-=(1+gamma)*lr*param.grad
        param.grad.data.zero_()
'''
NAG动量算法和经典动量算法差别就在BC梯度不同，动量算法更关注梯度下降方法的优化
'''
#AdaGrad算法通过参数来调整合适的学习率，能独立自动调整模型的参数学习率，非常适合处理稀疏数据
def sgd_adagrad(parameters,s,lr):
    eps=1e-10
    for param,s in zip(parameters,s):
        s[:]=s+(param.grad)**2
        div=lr/torch.sqrt(s+eps)*param.grad
        param.data=param.data-div
        param.grad.data.zero_()
#RMSProp算法在非凸背景下效果最好用指数加权的移动平均代替梯度平方和
def rmsprop(parameters,s,lr,alpha):
    eps=1e-10
    for param,sqr in zip(parameters,s):
        sqr[:]=alpha*sqr+(1-alpha)*param.grad**2
        div=lr/torch.sqrt(sqrt+eps)*param.grad
        param.data=param-div
        param.grad.data.zero_()
#Adam算法利用梯度的一阶矩估计和二阶矩估计动态调整每个参数学习率，主要优点在于经过偏置校正后，每一次迭代学习率都有一个确定范围
def adam(parameters,vs,s,lr,t,betal=0.9,beta2=0.999):
    eps=1e-8
    for param,v,sqr,in zip(parameters,vs,s):
        v[:]=betal*v+(1-betal)*param.grad
        sqr[:]=beta2*sqr+(1-beta2)*param.grad**2
        v_hat=v/(1-betal1**t)
        s_hat=sqr/(1-beta2**t)
        param.data=param.data-lr*v_hat/(torch.sqrt(s_hat)+eps)
        param.grad.data.zero_()
#Yogi算法
def yogi(parameters,vs,s,lr,t,betal=0.9,beta2=0.999):
    eps=1e-8
    for param,v,sqr in zip(parameters,vs,s):
        v[:]=betal1*v+(1-betal1)*param.grad
        sqr[:]=sqr+(1-beta2)*torch.sign(torch.square(param.grad)-sqr)*torch.square(param.grad)
        v_hat=v/(1-beta1**t)
        s_hat=sqr/(1-beta2**t)
        param.data=param.data-lr*v_hat/(torch.sqr(s_hat)+eps)
        param.grad.data.zero_()
# 优化算法实例
import numpy as np
import torch
from torchvision.datasets import MNIST
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import torch.nn.functional as F
import torch.optim as optim
from torch import nn
from torch.utils.tensorboard import SummaryWriter
import time
import matplotlib.pyplot as plt

# 定义预处理函数
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

# 定义模型
net = nn.Sequential(
    nn.Linear(784, 200),
    nn.ReLU(),
    nn.Linear(200, 10)
)

# 定义损失函数
loss_sgd = nn.CrossEntropyLoss()

# 下载数据，并对数据预处理
train_dataset = MNIST('../data/', train=True, transform=transform, download=True)

# 得到一个生成器，可节省内存
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)

# 初始化速度项（用于 momentum）
vs = []
for param in net.parameters():
    vs.append(torch.zeros_like(param.data))

# 定义优化器
def sgd(parameters, lr):
    for param in parameters:
        param.data -= lr * param.grad

def sgd_momentum(parameters, vs, lr, gamma):
    for param, v in zip(parameters, vs):
        v.data = gamma * v.data + lr * param.grad
        param.data -= v.data

# 开始训练
losses0 = []
losses1 = []
idx = 0
start = time.time()  # 计时开始

# 使用 SGD
for e in range(5):
    train_loss = 0
    for img, label in train_loader:
        # 展平 Img
        img = img.view(img.size(0), -1)
        # 正向传播
        out = net(img)
        loss = loss_sgd(out, label)
        # 反向传播
        net.zero_grad()
        loss.backward()
        sgd(net.parameters(), 1e-2)#更新参数
        # 记录误差
        train_loss += loss.item()
        if idx % 30 == 0:
            losses0.append(loss.item())
        idx += 1
    print('epoch:{}, Train Loss:{:.6f}'.format(e, train_loss / len(train_loader)))

end = time.time()
print('使用时间：{:.5f} s'.format(end - start))

# 使用 SGD Momentum
idx = 0
start = time.time()  # 计时开始
for e in range(5):
    train_loss = 0
    for img, label in train_loader:
        # 展平 Img
        img = img.view(img.size(0), -1)
        # 正向传播
        out = net(img)
        loss = loss_sgd(out, label)
        # 反向传播
        net.zero_grad()
        loss.backward()
        sgd_momentum(net.parameters(), vs, 1e-2, 0.9)
        # 记录误差
        train_loss += loss.item()
        if idx % 30 == 0:
            losses1.append(loss.item())
        idx += 1
    print('epoch:{}, Train Loss:{:.6f}'.format(e, train_loss / len(train_loader)))

end = time.time()
print('使用时间：{:.5f} s'.format(end - start))

# 绘图
x_axis = np.linspace(0, 5, len(losses0), endpoint=True)
plt.semilogy(x_axis, losses0, label='sgd')
plt.semilogy(x_axis, losses1, label='momentum')
plt.legend(loc='best')
plt.show()
'''
#单GPU加速
device=torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device1=torch.device("cuda:1")
for batch_idx,(img,label) in enumerate(train_loader)
    img=img.to(device)
    label=label.to(device)
#将网络放到GPU显存中
model=Net()
model.to(device)#使用序号为0的GPU
'''
'''
多GPU加速
对模型
net=torch.nn.DataParallel(model)
'''
print(torch.cuda.is_available())  # 应该输出 True
print(torch.cuda.get_device_name(0))  # 输出 GPU 名称
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from torch.utils.tensorboard import SummaryWriter

# 加载波士顿房价数据集
data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)

# 数据预处理
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

# 转换为 PyTorch 张量
data_tensor = torch.tensor(data, dtype=torch.float32)
target_tensor = torch.tensor(target, dtype=torch.float32).view(-1, 1)

# 创建数据集和数据加载器
dataset = TensorDataset(data_tensor, target_tensor)
train_loader = DataLoader(dataset, batch_size=128, shuffle=True)

# 定义网络
class Net(nn.Module):
    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):
        super(Net, self).__init__()
        self.layer1 = nn.Linear(in_dim, n_hidden_1)
        self.layer2 = nn.Linear(n_hidden_1, n_hidden_2)
        self.layer3 = nn.Linear(n_hidden_2, out_dim)

    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = self.layer3(x)
        return x

# 检查是否有多 GPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 实例化网络
model = Net(13, 16, 32, 1)
if torch.cuda.device_count() > 1:
    print(f"Let's use {torch.cuda.device_count()} GPUs")
    model = nn.DataParallel(model)  # 使用多 GPU

model.to(device)

# 选择优化器及损失函数
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
loss_func = nn.MSELoss()

# 模型训练并可视化损失值
# 模型训练并可视化损失值
writer = SummaryWriter(log_dir='logs')

for epoch in range(100):
    model.train()
    for batch_data, batch_labels in train_loader:
        # 将数据移动到设备上
        batch_data = batch_data.to(device)
        batch_labels = batch_labels.to(device)
        
        # 打印设备信息
        print(f"Batch data device: {batch_data.device}")
        print(f"Model device: {next(model.parameters()).device}")
        
        # 前向传播
        outputs = model(batch_data)
        loss = loss_func(outputs, batch_labels)
        
        # 反向传播和优化
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    # 记录损失
    
    writer.add_scalar('train_loss', loss.item(), epoch)
    print(f"Epoch [{epoch+1}/100], Loss: {loss.item():.4f}")

writer.close()
