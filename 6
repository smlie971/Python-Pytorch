import torch
import torch.nn as nn
#定义卷积运算
def cust_conv2d(X,K):
    #获取卷积核形状
    h,w=K.shape
    #初始化输出值Y
    Y=torch.zeros((X.shape[0]-h+1,X.shape[1]-w+1))
    #实现卷积运算 
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i,j]=(X[i:i+h,j:j+w]*K).sum()
    return Y
#定义输入以及卷积核
X=torch.tensor([[1.0,1.0,1.0,0,0],[0,1.0,1.0,1.0,0],[0,0,1.0,1.0,1.0],[0,0,1.0,1.0,1.0],[0,1.0,1.0,0,0]])
K=torch.tensor([[1,0,1],[0,1,0],[1,0,1]])
cust_conv2d(X,K)
#定义输入以及输出
X=torch.tensor([[10.0,10.0,10.0,0.0,0.0,0.0],[10.0,10.0,10.0,0.0,0.0,0.0],[10.0,10.0,10.0,0.0,0.0,0.0],[10.0,10.0,10.0,0.0,0.0,0.0],[10.0,10.0,10.0,0.0,0.0,0.0],[10.0,10.0,10.0,0.0,0.0,0.0]])
Y=torch.tensor([[0.0,30.0,30.0,0.0],[0.0,30.0,30.0,0.0],[0.0,30.0,30.0,0.0],[0.0,30.0,30.0,0.0]])
#训练卷积层
conv2d=nn.Conv2d(1,1,kernel_size=(3,3),bias=False)
#二维卷积使用四维输出和输入格式（批量大小，通道，高度，宽度）
X=X.reshape((1,1,6,6))
Y=Y.reshape((1,1,4,4))
lr=0.001
#定义损失函数
loss_fn=torch.nn.MSELoss()
for i in range(400):
    Y_pre=conv2d(X)#使用卷积层对输入 X 进行前向传播，得到预测值
    loss=loss_fn(Y_pre,Y)
    conv2d.zero_grad()
    loss.backward()
    #迭代卷积核
    conv2d.weight.data[:]-=lr*conv2d.weight.grad
    if(i+1)%100==0:
        print(f'epoch {i+1}，loss{loss.sum():.4f}')
conv2d.weight.data.reshape((3,3))
#定义多通道卷积运算函数
def corr2d_mutil_in(X,K):
    h,w=K.shape[1],K.shape[2]
    value=torch.zeros(X.shape[0]-h+1,X.shape[1]-w+1)
    for x,k in zip(X,K):
        value=value+cust_conv2d(X,K)#累加器，用于存储所有通道互相关结果的总和，每次迭代中，将当前通道的互相关结果加到 value 中
    return value
#使用CNNNet实现CIFAR10多分类
import torch
import torchvision
import torchvision.transforms as transforms
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=0)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=0)
classes=('plane','car','bird','cat','deer','dog','frog','horse','ship','truck')
#随机查看部分数据
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
#显示图像
def imshow(img):
    img=img/2+0.5#反归一化操作，将图像数据从 [-1, 1] 转换回 [0, 1]
    npimg=img.numpy()#将张量转换为 NumPy 数组
    plt.imshow(np.transpose(npimg,(1,2,0)))#转置数组的维度，从PyTorch 张量的形状 (C, H, W) 转换为Matplotlib 需要的图像数据形状 (H, W, C)
    plt.show()
#随机获取部分训练数据
dataiter=iter(trainloader)
images,labels=next(dataiter)
#显示图像
imshow(torchvision.utils.make_grid(images))
#打印标签
print(' '.join('%5s'% classes[labels[j]] for j in range(4)))#根据 labels 列表的索引获取的 4 个类别的名称，并且每个类别名称占用 5 个字符宽度,batch_size=4
import torch.nn as nn
import torch.nn.functional as F
#构建网络
device=torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
class CNNNet(nn.Module):
    def __init__(self):
        super(CNNNet,self).__init__()
        self.conv1=nn.Conv2d(in_channels=3,out_channels=16,kernel_size=5,stride=1)
        self.pool1=nn.MaxPool2d(kernel_size=2,stride=2)
        self.conv2=nn.Conv2d(in_channels=16,out_channels=36,kernel_size=3,stride=1)#36是设置的，比16多允许提取更复杂特征，确定最佳通道数（16，32，64）等
        self.pool2=nn.MaxPool2d(kernel_size=2,stride=2)
        self.fc1=nn.Linear(1296,128)
        self.fc2=nn.Linear(128,10)#前一层的128维特征向量映射到10个类别得分。这些得分可以用于计算损失函数（如交叉熵损失）并进行分类预测
        '''
        第一层卷积：输入32*32*3（图像大小32*32，通道数3）；卷积核：5*5，步幅1；输出：（32-5+1）*（32-5+1）*16=28*28*16，定义16个卷积核
        第一层池化：池化窗口：2*2，步幅2；输出：28/2*28/2*16=14*14*16
        第二层卷积：输入：14*14*16；卷积核：3*3，步幅1；输出：(14-3+1)*(14-3+1)*36=12*12*36，定义36个卷积核
        第二层池化：池化窗口：2*2，步幅2；输出：12/2*12/2*36=6*6*36
        全连接层输入特征数：6*6*36=1296
        128：通过实验和设计考虑确定的；10：输出特征数，即分类的类别数
        '''
    def forward(self,x):
        x=self.pool1(F.relu(self.conv1(x)))
        x=self.pool2(F.relu(self.conv2(x)))
        x=x.view(-1,36*6*6)
        x=F.relu(self.fc2(F.relu(self.fc1(x))))
        return x
net=CNNNet()
net=net.to(device)
#显示网络中定义了那些层
print(net)
#查看模型前四层
nn.Sequential(*list(net.children())[:4])
#选择优化器
import torch.optim as optim
criterion=nn.CrossEntropyLoss()
optimizer=optim.SGD(net.parameters(),lr=0.001,momentum=0.9)
#训练模型
for epoch in range(11):
    running_loss=0.0
    for i,data in enumerate(trainloader,0):
    #获取训练数据
       inputs,labels=data
       inputs,labels=inputs.to(device),labels.to(device)
    #权重参数梯度清零
       optimizer.zero_grad()
    #正向及反向传播
       outputs=net(inputs)
       loss=criterion(outputs,labels)
       loss.backward()
       optimizer.step()
    #显示损失值
       running_loss+=loss.item()#loss.item()获取当前批次的损失值（loss是一个张量，.item()将其转换为一个Python标量）
       if i %2000==1999:
           print('[%d,%5d] loss:%.3f' %(epoch+1,i+1,running_loss/2000))#i % 2000 == 1999表示每2000个批次执行一次打印操作，[%d, %5d]用于格式化输出当前的训练轮数和批次索引
           running_loss=0.0
print('Finished Training')
#测试模型
correct=0
total=0
with torch.no_grad():
    for data in testloader:
        images,labels=data
        images,labels=images.to(device),labels.to(device)
        outputs=net(images)
        _,predicted=torch.max(outputs.data,1)#找到每个输出的最大值对应的索引，即预测的类别
        total+=labels.size(0)#累加总样本数
        correct+=(predicted==labels).sum().item()#累加正确预测的样本数
print('Accuracy of the network on the 10000 test images:%d %%' % (100*correct/total))#%% 表示插入一个百分号 %
class_correct=list(0. for i in range(10))
class_total=list(0. for i in range(10))
with torch.no_grad():
    for data in testloader:
        images,labels=data
        images,labels=images.to(device),labels.to(device)
        outputs=net(images)
        _,predicted=torch.max(outputs,1)
        c=(predicted==labels).squeeze()#比较预测值和真实值，得到布尔型张量，表示每个样本是否预测正确
        for i in range(4):
            label=labels[i]
            class_correct[label.item()] += c[i].item()#如果预测正确，累加到对应类别的正确预测数，用.item()将张量转换为整数
            class_total[label.item()] += 1
for i in range(10):
    print('Accuracy of %5s : %2d %%' % (classes[i], 100*class_correct[i]/class_total[i]))
    print("net_gvp have {} parameters in total".format(sum(x.numel() for x in net.parameters())))
#像keras一样显示各层参数
#先定义汇总各层网络参数的函数
import collections
import torch
import torch
import torch.nn as nn
import collections
def paras_summary(input_size, model):
    summary = collections.OrderedDict()#存储模型的参数摘要
    hooks = []#保存注册的钩子函数
    def register_hook(module):
        def hook(module, input, output):
            class_name = str(module.__class__).split('.')[-1].split("'")[0]#获取当前模块的类名，取列表的最后一个元素使用 . 作为分隔符，
            module_idx = len(summary)
            m_key = '%s-%i' % (class_name, module_idx + 1)#生成一个唯一的键m_key，键的格式为类名-索引
            summary[m_key] = collections.OrderedDict()#为当前模块创建一个新的有序字典
            summary[m_key]['input_shape'] = list(input[0].size())
            summary[m_key]['input_shape'][0] = -1#记录当前模块的输入形状，并将批量大小设置为-1以表示可变批量大小
            summary[m_key]['output_shape'] = list(output.size())
            summary[m_key]['output_shape'][0] = -1
            params = 0
            if hasattr(module, 'weight'):
                params += torch.prod(torch.LongTensor(list(module.weight.size())))
                if module.weight.requires_grad:#如果权重参数需要梯度更新
                    summary[m_key]['trainable'] = True
                else:
                    summary[m_key]['trainable'] = False
            if hasattr(module, 'bias'):
                params += torch.prod(torch.LongTensor(list(module.bias.size())))#如果模块有偏置参数，计算偏置的参数数量并累加到params
            summary[m_key]['nb_params'] = params#计算得到的可训练参数数量存储到当前模块的摘要中
        if not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList):
            hooks.append(module.register_forward_hook(hook))#注册前向钩子函数，并将钩子对象添加到hooks列表中
    if isinstance(input_size[0], (list, tuple)):
        x = [torch.rand(1, *in_size) for in_size in input_size]
    else:
        x = torch.rand(1, *input_size)
    model.apply(register_hook)#对模型中的每个模块应用register_hook函数，注册钩子。
    model(x)#对模型进行一次前向传播，触发注册的钩子函数，收集每个模块的输入输出信息
    for h in hooks:
        h.remove()#遍历hooks列表，移除所有注册的钩子函数，以避免对后续操作产生影响
    return summary
#确定输入及实例化模型
net=CNNNet()
#输入格式为[c,h,w]
input_size=[3,32,32]
paras_summary(input_size,net)

#使用LeNet实现CIFAR10多分类
import torch
import torchvision
import torchvision.transforms as transforms
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=0)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=0)
classes=('plane','car','bird','cat','deer','dog','frog','horse','ship','truck')

#随机获取部分训练数据
dataiter=iter(trainloader)
images,labels=next(dataiter)
#显示图像
imshow(torchvision.utils.make_grid(images))
#打印标签
print(' '.join('%5s'% classes[labels[j]] for j in range(4)))#根据 labels 列表的索引获取的 4 个类别的名称，并且每个类别名称占用 5 个字符宽度,batch_size=4
import torch.nn as nn
import torch.nn.functional as F
#构建网络
device=torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
#LeNet模型生成代码
class LeNet(nn.Module):
    def __init__(self):
        super(LeNet,self).__init__()
        self.conv1=nn.Conv2d(3,6,5)
        self.conv2=nn.Conv2d(6,16,5)
        self.fc1=nn.Linear(16*5*5,120)
        self.fc2=nn.Linear(120,84)
        self.fc3=nn.Linear(84,10)
        '''
        卷积层数数量计算：总参数数量=（输入通道数*输出通道数*卷积核高度*卷积核宽度）+偏置参数数量
        每个卷积核参数数量=（输入通道数*卷积核高度*卷积核宽度）+偏置参数数量
        全连接层：参数数量=输入特征数*输出特征数+偏置项
        偏置数量取决于层的输出通道或者输出特征数
        '''
    def forward(self,x):
        out=F.relu(self.conv1(x))
        out=F.max_pool2d(out,2)
        out=F.relu(self.conv2(out))
        out=F.max_pool2d(out,2)
        out=out.view(out.size(0),-1)
        out=F.relu(self.fc1(out))
        out=F.relu(self.fc2(out))
        out=self.fc3(out)
        return out
net=LeNet()
net=net.to(device)
#显示网络中定义了那些层
print(net)
#查看模型前四层
nn.Sequential(*list(net.children())[:4])
#选择优化器
import torch.optim as optim
criterion=nn.CrossEntropyLoss()
optimizer=optim.SGD(net.parameters(),lr=0.001,momentum=0.9)
#训练模型
for epoch in range(11):
    running_loss=0.0
    for i,data in enumerate(trainloader,0):
    #获取训练数据
       inputs,labels=data
       inputs,labels=inputs.to(device),labels.to(device)
    #权重参数梯度清零
       optimizer.zero_grad()
    #正向及反向传播
       outputs=net(inputs)
       loss=criterion(outputs,labels)
       loss.backward()
       optimizer.step()
    #显示损失值
       running_loss+=loss.item()#loss.item()获取当前批次的损失值（loss是一个张量，.item()将其转换为一个Python标量）
       if i %2000==1999:
           print('[%d,%5d] loss:%.3f' %(epoch+1,i+1,running_loss/2000))#i % 2000 == 1999表示每2000个批次执行一次打印操作，[%d, %5d]用于格式化输出当前的训练轮数和批次索引
           running_loss=0.0
print('Finished Training')
#测试模型
correct=0
total=0
with torch.no_grad():
    for data in testloader:
        images,labels=data
        images,labels=images.to(device),labels.to(device)
        outputs=net(images)
        _,predicted=torch.max(outputs.data,1)#找到每个输出的最大值对应的索引，即预测的类别
        total+=labels.size(0)#累加总样本数
        correct+=(predicted==labels).sum().item()#累加正确预测的样本数
print('Accuracy of the network on the 10000 test images:%d %%' % (100*correct/total))#%% 表示插入一个百分号 %
class_correct=list(0. for i in range(10))
class_total=list(0. for i in range(10))
with torch.no_grad():
    for data in testloader:
        images,labels=data
        images,labels=images.to(device),labels.to(device)
        outputs=net(images)
        _,predicted=torch.max(outputs,1)
        c=(predicted==labels).squeeze()#比较预测值和真实值，得到布尔型张量，表示每个样本是否预测正确
        for i in range(4):
            label=labels[i]
            class_correct[label.item()] += c[i].item()#如果预测正确，累加到对应类别的正确预测数，用.item()将张量转换为整数
            class_total[label.item()] += 1
for i in range(10):
    print('Accuracy of %5s : %2d %%' % (classes[i], 100*class_correct[i]/class_total[i]))
    print("net_gvp have {} parameters in total".format(sum(x.numel() for x in net.parameters())))
#像keras一样显示各层参数
#先定义汇总各层网络参数的函数
import collections
import torch
import torch
import torch.nn as nn
import collections
def paras_summary(input_size, model):
    summary = collections.OrderedDict()#存储模型的参数摘要
    hooks = []#保存注册的钩子函数
    def register_hook(module):
        def hook(module, input, output):
            class_name = str(module.__class__).split('.')[-1].split("'")[0]#获取当前模块的类名，取列表的最后一个元素使用 . 作为分隔符，
            module_idx = len(summary)
            m_key = '%s-%i' % (class_name, module_idx + 1)#生成一个唯一的键m_key，键的格式为类名-索引
            summary[m_key] = collections.OrderedDict()#为当前模块创建一个新的有序字典
            summary[m_key]['input_shape'] = list(input[0].size())
            summary[m_key]['input_shape'][0] = -1#记录当前模块的输入形状，并将批量大小设置为-1以表示可变批量大小
            summary[m_key]['output_shape'] = list(output.size())
            summary[m_key]['output_shape'][0] = -1
            params = 0
            if hasattr(module, 'weight'):
                params += torch.prod(torch.LongTensor(list(module.weight.size())))
                if module.weight.requires_grad:#如果权重参数需要梯度更新
                    summary[m_key]['trainable'] = True
                else:
                    summary[m_key]['trainable'] = False
            if hasattr(module, 'bias'):
                params += torch.prod(torch.LongTensor(list(module.bias.size())))#如果模块有偏置参数，计算偏置的参数数量并累加到params
            summary[m_key]['nb_params'] = params#计算得到的可训练参数数量存储到当前模块的摘要中
        if not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList):
            hooks.append(module.register_forward_hook(hook))#注册前向钩子函数，并将钩子对象添加到hooks列表中
    if isinstance(input_size[0], (list, tuple)):
        x = [torch.rand(1, *in_size) for in_size in input_size]
    else:
        x = torch.rand(1, *input_size)
    model.apply(register_hook)#对模型中的每个模块应用register_hook函数，注册钩子。
    model(x)#对模型进行一次前向传播，触发注册的钩子函数，收集每个模块的输入输出信息
    for h in hooks:
        h.remove()#遍历hooks列表，移除所有注册的钩子函数，以避免对后续操作产生影响
    return summary
#确定输入及实例化模型
net=CNNNet()
#输入格式为[c,h,w]
input_size=[3,32,32]
paras_summary(input_size,net)
#使用Net（全局平均池化层）实现CIFAR10多分类
import torch
import torchvision
import torchvision.transforms as transforms
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=0)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=0)
classes=('plane','car','bird','cat','deer','dog','frog','horse','ship','truck')
#随机查看部分数据
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
#显示图像
def imshow(img):
    img=img/2+0.5#反归一化操作，将图像数据从 [-1, 1] 转换回 [0, 1]
    npimg=img.numpy()#将张量转换为 NumPy 数组
    plt.imshow(np.transpose(npimg,(1,2,0)))#转置数组的维度，从PyTorch 张量的形状 (C, H, W) 转换为Matplotlib 需要的图像数据形状 (H, W, C)
    plt.show()
#随机获取部分训练数据
dataiter=iter(trainloader)
images,labels=next(dataiter)
#显示图像
imshow(torchvision.utils.make_grid(images))
#打印标签
print(' '.join('%5s'% classes[labels[j]] for j in range(4)))#根据 labels 列表的索引获取的 4 个类别的名称，并且每个类别名称占用 5 个字符宽度,batch_size=4
import torch.nn as nn
import torch.nn.functional as F
#构建网络
device=torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
#使用全局平均池化层
device=torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
class Net(nn.Module):
    '''
    通过卷积层和池化层提取特征图像，并使用全局平均池化层减少特征维度，最后通过全连接层进行分类，减少了全连接层数量，简化了网络结构，通过全局平均池化降低了过拟合风险
    '''
    def __init__(self):
        super(Net,self).__init__()
        self.conv1=nn.Conv2d(3,16,5)
        self.pool1=nn.MaxPool2d(2,2)
        self.conv2=nn.Conv2d(16,36,5)
        #self.fc1=nn.Linear(15*5*5,120)
        self.pool2=nn.MaxPool2d(2,2)
        #使用全局平均池化层
        self.aap=nn.AdaptiveAvgPool2d(1)
        self.fc3=nn.Linear(36,10)
    def forward(self,x):
        x=self.pool1(F.relu(self.conv1(x)))
        x=self.pool2(F.relu(self.conv2(x)))
        x=self.aap(x)
        x=x.view(x.shape[0],-1)
        x=self.fc3(x)
        return x
net=Net()
net=net.to(device)

#显示网络中定义了那些层
print(net)
#查看模型前四层
nn.Sequential(*list(net.children())[:4])
#选择优化器
import torch.optim as optim
criterion=nn.CrossEntropyLoss()
optimizer=optim.SGD(net.parameters(),lr=0.001,momentum=0.9)
#训练模型
for epoch in range(11):
    running_loss=0.0
    for i,data in enumerate(trainloader,0):
    #获取训练数据
       inputs,labels=data
       inputs,labels=inputs.to(device),labels.to(device)
    #权重参数梯度清零
       optimizer.zero_grad()
    #正向及反向传播
       outputs=net(inputs)
       loss=criterion(outputs,labels)
       loss.backward()
       optimizer.step()
    #显示损失值
       running_loss+=loss.item()#loss.item()获取当前批次的损失值（loss是一个张量，.item()将其转换为一个Python标量）
       if i %2000==1999:
           print('[%d,%5d] loss:%.3f' %(epoch+1,i+1,running_loss/2000))#i % 2000 == 1999表示每2000个批次执行一次打印操作，[%d, %5d]用于格式化输出当前的训练轮数和批次索引
           running_loss=0.0
print('Finished Training')
#测试模型
correct=0
total=0
with torch.no_grad():
    for data in testloader:
        images,labels=data
        images,labels=images.to(device),labels.to(device)
        outputs=net(images)
        _,predicted=torch.max(outputs.data,1)#找到每个输出的最大值对应的索引，即预测的类别
        total+=labels.size(0)#累加总样本数
        correct+=(predicted==labels).sum().item()#累加正确预测的样本数
print('Accuracy of the network on the 10000 test images:%d %%' % (100*correct/total))#%% 表示插入一个百分号 %
class_correct=list(0. for i in range(10))
class_total=list(0. for i in range(10))
with torch.no_grad():
    for data in testloader:
        images,labels=data
        images,labels=images.to(device),labels.to(device)
        outputs=net(images)
        _,predicted=torch.max(outputs,1)
        c=(predicted==labels).squeeze()#比较预测值和真实值，得到布尔型张量，表示每个样本是否预测正确
        for i in range(4):
            label=labels[i]
            class_correct[label.item()] += c[i].item()#如果预测正确，累加到对应类别的正确预测数，用.item()将张量转换为整数
            class_total[label.item()] += 1
for i in range(10):
    print('Accuracy of %5s : %2d %%' % (classes[i], 100*class_correct[i]/class_total[i]))
    print("net_gvp have {} parameters in total".format(sum(x.numel() for x in net.parameters())))
#像keras一样显示各层参数
#先定义汇总各层网络参数的函数
import collections
import torch
import torch
import torch.nn as nn
import collections
def paras_summary(input_size, model):
    summary = collections.OrderedDict()#存储模型的参数摘要
    hooks = []#保存注册的钩子函数
    def register_hook(module):
        def hook(module, input, output):
            class_name = str(module.__class__).split('.')[-1].split("'")[0]#获取当前模块的类名，取列表的最后一个元素使用 . 作为分隔符，
            module_idx = len(summary)
            m_key = '%s-%i' % (class_name, module_idx + 1)#生成一个唯一的键m_key，键的格式为类名-索引
            summary[m_key] = collections.OrderedDict()#为当前模块创建一个新的有序字典
            summary[m_key]['input_shape'] = list(input[0].size())
            summary[m_key]['input_shape'][0] = -1#记录当前模块的输入形状，并将批量大小设置为-1以表示可变批量大小
            summary[m_key]['output_shape'] = list(output.size())
            summary[m_key]['output_shape'][0] = -1
            params = 0
            if hasattr(module, 'weight'):
                params += torch.prod(torch.LongTensor(list(module.weight.size())))
                if module.weight.requires_grad:#如果权重参数需要梯度更新
                    summary[m_key]['trainable'] = True
                else:
                    summary[m_key]['trainable'] = False
            if hasattr(module, 'bias'):
                params += torch.prod(torch.LongTensor(list(module.bias.size())))#如果模块有偏置参数，计算偏置的参数数量并累加到params
            summary[m_key]['nb_params'] = params#计算得到的可训练参数数量存储到当前模块的摘要中
        if not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList):
            hooks.append(module.register_forward_hook(hook))#注册前向钩子函数，并将钩子对象添加到hooks列表中
    if isinstance(input_size[0], (list, tuple)):
        x = [torch.rand(1, *in_size) for in_size in input_size]
    else:
        x = torch.rand(1, *input_size)
    model.apply(register_hook)#对模型中的每个模块应用register_hook函数，注册钩子。
    model(x)#对模型进行一次前向传播，触发注册的钩子函数，收集每个模块的输入输出信息
    for h in hooks:
        h.remove()#遍历hooks列表，移除所有注册的钩子函数，以避免对后续操作产生影响
    return summary
#确定输入及实例化模型
net=Net()
#输入格式为[c,h,w]
input_size=[3,32,32]
paras_summary(input_size,net)
import torch
import torchvision
import torchvision.transforms as transforms
from collections import Counter
import numpy as np
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=0)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=0)
classes=('plane','car','bird','cat','deer','dog','frog','horse','ship','truck')
import torch.nn as nn
import torch.nn.functional as F
#构建网络
device=torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
#采用集成方法，类似于投票机制
class Net(nn.Module):
    '''
    通过卷积层和池化层提取特征图像，并使用全局平均池化层减少特征维度，最后通过全连接层进行分类，减少了全连接层数量，简化了网络结构，通过全局平均池化降低了过拟合风险
    '''
    def __init__(self):
        super(Net,self).__init__()
        self.conv1=nn.Conv2d(3,16,5)
        self.pool1=nn.MaxPool2d(2,2)
        self.conv2=nn.Conv2d(16,36,5)
        #self.fc1=nn.Linear(15*5*5,120)
        self.pool2=nn.MaxPool2d(2,2)
        #使用全局平均池化层
        self.aap=nn.AdaptiveAvgPool2d(1)
        self.fc3=nn.Linear(36,10)
    def forward(self,x):
        x=self.pool1(F.relu(self.conv1(x)))
        x=self.pool2(F.relu(self.conv2(x)))
        x=self.aap(x)
        x=x.view(x.shape[0],-1)
        x=self.fc3(x)
        return x
net1 = Net().to(device)
class LeNet(nn.Module):
    def __init__(self):
        super(LeNet,self).__init__()
        self.conv1=nn.Conv2d(3,6,5)
        self.conv2=nn.Conv2d(6,16,5)
        self.fc1=nn.Linear(16*5*5,120)
        self.fc2=nn.Linear(120,84)
        self.fc3=nn.Linear(84,10)
        '''
        卷积层数数量计算：总参数数量=（输入通道数*输出通道数*卷积核高度*卷积核宽度）+偏置参数数量
        每个卷积核参数数量=（输入通道数*卷积核高度*卷积核宽度）+偏置参数数量
        全连接层：参数数量=输入特征数*输出特征数+偏置项
        偏置数量取决于层的输出通道或者输出特征数
        '''
    def forward(self,x):
        out=F.relu(self.conv1(x))
        out=F.max_pool2d(out,2)
        out=F.relu(self.conv2(out))
        out=F.max_pool2d(out,2)
        out=out.view(out.size(0),-1)
        out=F.relu(self.fc1(out))
        out=F.relu(self.fc2(out))
        out=self.fc3(out)
        return out
net2 = LeNet().to(device)
class CNNNet(nn.Module):
    def __init__(self):
        super(CNNNet,self).__init__()
        self.conv1=nn.Conv2d(in_channels=3,out_channels=16,kernel_size=5,stride=1)
        self.pool1=nn.MaxPool2d(kernel_size=2,stride=2)
        self.conv2=nn.Conv2d(in_channels=16,out_channels=36,kernel_size=3,stride=1)#36是设置的，比16多允许提取更复杂特征，确定最佳通道数（16，32，64）等
        self.pool2=nn.MaxPool2d(kernel_size=2,stride=2)
        self.fc1=nn.Linear(1296,128)
        self.fc2=nn.Linear(128,10)#前一层的128维特征向量映射到10个类别得分。这些得分可以用于计算损失函数（如交叉熵损失）并进行分类预测
        '''
        第一层卷积：输入32*32*3（图像大小32*32，通道数3）；卷积核：5*5，步幅1；输出：（32-5+1）*（32-5+1）*16=28*28*16，定义16个卷积核
        第一层池化：池化窗口：2*2，步幅2；输出：28/2*28/2*16=14*14*16
        第二层卷积：输入：14*14*16；卷积核：3*3，步幅1；输出：(14-3+1)*(14-3+1)*36=12*12*36，定义36个卷积核
        第二层池化：池化窗口：2*2，步幅2；输出：12/2*12/2*36=6*6*36
        全连接层输入特征数：6*6*36=1296
        128：通过实验和设计考虑确定的；10：输出特征数，即分类的类别数
        '''
    def forward(self,x):
        x=self.pool1(F.relu(self.conv1(x)))
        x=self.pool2(F.relu(self.conv2(x)))
        x=x.view(-1,36*6*6)
        x=F.relu(self.fc2(F.relu(self.fc1(x))))
        return x
net3 = CNNNet().to(device)
mlps=[net1.to(device),net2.to(device),net3.to(device),]
optimizer=torch.optim.Adam([{"params":mlp.parameters()} for mlp in mlps],lr=0.001)
loss_function=nn.CrossEntropyLoss()
for ep in range(100):
    for img,label in trainloader:
        img,label=img.to(device),label.to(device)
        optimizer.zero_grad()
        for mlp in mlps:#遍历每个模型
            mlp.train()#训练模式
            out=mlp(img)#前向传播得到输出
            loss=loss_function(out,label)
            loss.backward()
        optimizer.step()#更新参数
    pre=[]
    vote_correct=0
    mlps_correct=[0 for i in range(len(mlps))]
    for img,label in testloader:
        img,label=img.to(device),label.to(device)
        for i ,mlp in enumerate(mlps):#遍历每个模型进行评估
            mlp.eval()#评估模式
            out=mlp(img)
            _,prediction=torch.max(out,1)#按行取最大值，1: 指定在维度 1 上进行操作，torch.max 函数来获取张量 out 在指定维度上的最大值及其索引
             # 将预测结果转换为numpy数组，并与真实标签比较，统计正确数量
            pre_num=prediction.cpu().numpy()
            mlps_correct[i]+=(pre_num==label.cpu().numpy()).sum()
            pre.append(pre_num)
        arr=np.array(pre)
        pre.clear()
        #对每个样本的预测结果进行投票，取出现次数最多的类别作为最终预测
        result=[Counter(arr[:,i]).most_common(1)[0][0] for i in range(4)]
        '''
        arr[:, i]所有模型对第 i 个样本的预测结果
        Counter(arr[:, i]) 返回一个字典类型的对象，其中键是元素，值是该元素出现的次数
        most_common(1) 方法返回出现次数最多的元素及其出现次数，元组的第一个元素是出现次数最多的值，第二个元素是出现次数
        如果 Counter 对象是 Counter({2: 3, 1: 2})，那么 most_common(1) 返回 [(2, 3)]
        [0] 用于获取 most_common(1) 返回的列表中的第一个元组
        [0][0] 用于获取该元组的第一个元素，即出现次数最多的值。
        '''
         # 统计投票预测正确的数量
        vote_correct+=(result==label.cpu().numpy()).sum()
    print("epoch:"+ str(ep)+"集成模型的正确率"+str(vote_correct/len(testloader)))
    for idx,correct in enumerate(mlps_correct): # 打印每个单独模型的正确率
        print("模型"+str(idx)+"的正确率"+str(correct/len(testloader)))
        
    #使用VGG16实现CIFAR10多分类
import torch
import torchvision
import torchvision.transforms as transforms
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=0)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=0)
classes=('plane','car','bird','cat','deer','dog','frog','horse','ship','truck')

import torch.nn as nn
import torch.nn.functional as F
#构建网络
device=torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
#VGG16模型生成代码
cfg={'VGG16':[64,64,'M',128,128,'M',256,256,256,'M',512,512,512,'M',
              512,512,512,'M'],
    'VGG19':[64,64,'M',128,128,'M',256,256,256,256,'M',512,512,512,512,'M',512,512,512,512,'M'],}
'''
[64, 64, 'M']：表示两个卷积层，输出通道数为 64，后面跟着一个最大池化层。
[128, 128, 'M']：表示两个卷积层，输出通道数为 128，后面跟着一个最大池化层。
[256, 256, 256, 'M']：表示三个卷积层，输出通道数为 256，后面跟着一个最大池化层。
[512, 512, 512, 'M']：表示三个卷积层，输出通道数为 512，后面跟着一个最大池化层。
[512, 512, 512, 'M']：表示三个卷积层，输出通道数为 512，后面跟着一个最大池化层。
[256, 256, 256, 256, 'M']：表示四个卷积层，输出通道数为 256，后面跟着一个最大池化层。
[512, 512, 512, 512, 'M']：表示四个卷积层，输出通道数为 512，后面跟着一个最大池化层。
[512, 512, 512, 512, 'M']：表示四个卷积层，输出通道数为 512，后面跟着一个最大池化层。
'''
class VGG(nn.Module):
    def __init__(self,vgg_name):
        super(VGG,self).__init__()
        self.feature=self._make_layers(cfg[vgg_name])
        self.classifier=nn.Linear(512,10)
    def forward(self,x):
        out=self.feature(x)
        out=out.view(out.size(0),-1)
        out=self.classifier(out)
        return out
    def _make_layers(self,cfg): # 根据配置字典生成网络层，层生成方法 _make_layers
        layers=[]
        in_channels=3
        for x in cfg:
            if x=='M':
                layers+=[nn.MaxPool2d(kernel_size=2,stride=2)]
            else:
                layers+=[nn.Conv2d(in_channels,x,kernel_size=3,padding=1),
                        nn.BatchNorm2d(x),
                        nn.ReLU(inplace=True)]
                in_channels=x
        layers+=[nn.AvgPool2d(kernel_size=1,stride=1)]
        return nn.Sequential(*layers)

net=VGG('VGG16')
net=net.to(device)
#显示网络中定义了那些层
print(net)
#查看模型前四层
nn.Sequential(*list(net.children())[:4])
#选择优化器
import torch.optim as optim
criterion=nn.CrossEntropyLoss()
optimizer=optim.SGD(net.parameters(),lr=0.001,momentum=0.9)
#训练模型
for epoch in range(10):
    running_loss=0.0
    for i,data in enumerate(trainloader,0):
    #获取训练数据
       inputs,labels=data
       inputs,labels=inputs.to(device),labels.to(device)
    #权重参数梯度清零
       optimizer.zero_grad()
    #正向及反向传播
       outputs=net(inputs)
       loss=criterion(outputs,labels)
       loss.backward()
       optimizer.step()
    #显示损失值
       running_loss+=loss.item()#loss.item()获取当前批次的损失值（loss是一个张量，.item()将其转换为一个Python标量）
       if i %2000==1999:
           print('[%d,%5d] loss:%.3f' %(epoch+1,i+1,running_loss/2000))#i % 2000 == 1999表示每2000个批次执行一次打印操作，[%d, %5d]用于格式化输出当前的训练轮数和批次索引
           running_loss=0.0
print('Finished Training')
#测试模型
correct=0
total=0
with torch.no_grad():
    for data in testloader:
        images,labels=data
        images,labels=images.to(device),labels.to(device)
        outputs=net(images)
        _,predicted=torch.max(outputs.data,1)#找到每个输出的最大值对应的索引，即预测的类别
        total+=labels.size(0)#累加总样本数
        correct+=(predicted==labels).sum().item()#累加正确预测的样本数
print('Accuracy of the network on the 10000 test images:%d %%' % (100*correct/total))#%% 表示插入一个百分号 %
class_correct=list(0. for i in range(10))
class_total=list(0. for i in range(10))
with torch.no_grad():
    for data in testloader:
        images,labels=data
        images,labels=images.to(device),labels.to(device)
        outputs=net(images)
        _,predicted=torch.max(outputs,1)
        c=(predicted==labels).squeeze()#比较预测值和真实值，得到布尔型张量，表示每个样本是否预测正确
        for i in range(4):
            label=labels[i]
            class_correct[label.item()] += c[i].item()#如果预测正确，累加到对应类别的正确预测数，用.item()将张量转换为整数
            class_total[label.item()] += 1
for i in range(10):
    print('Accuracy of %5s : %2d %%' % (classes[i], 100*class_correct[i]/class_total[i]))

    print("net_gvp have {} parameters in total".format(sum(x.numel() for x in net.parameters())))
        
    #使用VGG19实现CIFAR10多分类
import torch
import torchvision
import torchvision.transforms as transforms
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=0)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=0)
classes=('plane','car','bird','cat','deer','dog','frog','horse','ship','truck')

import torch.nn as nn
import torch.nn.functional as F
#构建网络
device=torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
#VGG16模型生成代码
cfg={'VGG16':[64,64,'M',128,128,'M',256,256,256,'M',512,512,512,'M',
              512,512,512,'M'],
    'VGG19':[64,64,'M',128,128,'M',256,256,256,256,'M',512,512,512,512,'M',512,512,512,512,'M'],}
'''
[64, 64, 'M']：表示两个卷积层，输出通道数为 64，后面跟着一个最大池化层。
[128, 128, 'M']：表示两个卷积层，输出通道数为 128，后面跟着一个最大池化层。
[256, 256, 256, 'M']：表示三个卷积层，输出通道数为 256，后面跟着一个最大池化层。
[512, 512, 512, 'M']：表示三个卷积层，输出通道数为 512，后面跟着一个最大池化层。
[512, 512, 512, 'M']：表示三个卷积层，输出通道数为 512，后面跟着一个最大池化层。
[256, 256, 256, 256, 'M']：表示四个卷积层，输出通道数为 256，后面跟着一个最大池化层。
[512, 512, 512, 512, 'M']：表示四个卷积层，输出通道数为 512，后面跟着一个最大池化层。
[512, 512, 512, 512, 'M']：表示四个卷积层，输出通道数为 512，后面跟着一个最大池化层。
'''
class VGG(nn.Module):
    def __init__(self,vgg_name):
        super(VGG,self).__init__()
        self.feature=self._make_layers(cfg[vgg_name])
        self.classifier=nn.Linear(512,10)
    def forward(self,x):
        out=self.feature(x)
        out=out.view(out.size(0),-1)
        out=self.classifier(out)
        return out
    def _make_layers(self,cfg): # 根据配置字典生成网络层，层生成方法 _make_layers
        layers=[]
        in_channels=3
        for x in cfg:
            if x=='M':
                layers+=[nn.MaxPool2d(kernel_size=2,stride=2)]
            else:
                layers+=[nn.Conv2d(in_channels,x,kernel_size=3,padding=1),
                        nn.BatchNorm2d(x),
                        nn.ReLU(inplace=True)]
                in_channels=x
        layers+=[nn.AvgPool2d(kernel_size=1,stride=1)]
        return nn.Sequential(*layers)

net=VGG('VGG19')
net=net.to(device)
#显示网络中定义了那些层
print(net)
#查看模型前四层
nn.Sequential(*list(net.children())[:4])
#选择优化器
import torch.optim as optim
criterion=nn.CrossEntropyLoss()
optimizer=optim.SGD(net.parameters(),lr=0.001,momentum=0.9)
#训练模型
for epoch in range(10):
    running_loss=0.0
    for i,data in enumerate(trainloader,0):
    #获取训练数据
       inputs,labels=data
       inputs,labels=inputs.to(device),labels.to(device)
    #权重参数梯度清零
       optimizer.zero_grad()
    #正向及反向传播
       outputs=net(inputs)
       loss=criterion(outputs,labels)
       loss.backward()
       optimizer.step()
    #显示损失值
       running_loss+=loss.item()#loss.item()获取当前批次的损失值（loss是一个张量，.item()将其转换为一个Python标量）
       if i %2000==1999:
           print('[%d,%5d] loss:%.3f' %(epoch+1,i+1,running_loss/2000))#i % 2000 == 1999表示每2000个批次执行一次打印操作，[%d, %5d]用于格式化输出当前的训练轮数和批次索引
           running_loss=0.0
print('Finished Training')
#测试模型
correct=0
total=0
with torch.no_grad():
    for data in testloader:
        images,labels=data
        images,labels=images.to(device),labels.to(device)
        outputs=net(images)
        _,predicted=torch.max(outputs.data,1)#找到每个输出的最大值对应的索引，即预测的类别
        total+=labels.size(0)#累加总样本数
        correct+=(predicted==labels).sum().item()#累加正确预测的样本数
print('Accuracy of the network on the 10000 test images:%d %%' % (100*correct/total))#%% 表示插入一个百分号 %
class_correct=list(0. for i in range(10))
class_total=list(0. for i in range(10))
with torch.no_grad():
    for data in testloader:
        images,labels=data
        images,labels=images.to(device),labels.to(device)
        outputs=net(images)
        _,predicted=torch.max(outputs,1)
        c=(predicted==labels).squeeze()#比较预测值和真实值，得到布尔型张量，表示每个样本是否预测正确
        for i in range(4):
            label=labels[i]
            class_correct[label.item()] += c[i].item()#如果预测正确，累加到对应类别的正确预测数，用.item()将张量转换为整数
            class_total[label.item()] += 1
for i in range(10):
    print('Accuracy of %5s : %2d %%' % (classes[i], 100*class_correct[i]/class_total[i]))

    print("net_gvp have {} parameters in total".format(sum(x.numel() for x in net.parameters())))
    
